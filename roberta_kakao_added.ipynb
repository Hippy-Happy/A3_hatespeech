{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"roberta_kakao_added.ipynb","provenance":[],"authorship_tag":"ABX9TyPofxLjukjk96tnomlOmh8D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"959d71a293b54714b33a12c0275c5a06":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef800a642e9448c1b75c2bbed640a4bb","IPY_MODEL_992736e6a6a846e484d0dfca5a0e517c","IPY_MODEL_931ce25b47b84659abff4554c90b02e0"],"layout":"IPY_MODEL_5c0cc41806de4341822c60ca72af02f2"}},"ef800a642e9448c1b75c2bbed640a4bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_35446649e9af4501b39a25a4fb4db86c","placeholder":"​","style":"IPY_MODEL_67ac93e37d42457faf3c28979120da25","value":" 45%"}},"992736e6a6a846e484d0dfca5a0e517c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_789e8061d1684b298c221eeb0ea6322d","max":300,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae2de34d8f634bffafad6c89eaa61389","value":136}},"931ce25b47b84659abff4554c90b02e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40aa9c3042654b4c8058278c6707ff79","placeholder":"​","style":"IPY_MODEL_6fb7c902bde74dbf8586840535add000","value":" 136/300 [10:33&lt;25:29,  9.32s/it]"}},"5c0cc41806de4341822c60ca72af02f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35446649e9af4501b39a25a4fb4db86c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67ac93e37d42457faf3c28979120da25":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"789e8061d1684b298c221eeb0ea6322d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae2de34d8f634bffafad6c89eaa61389":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40aa9c3042654b4c8058278c6707ff79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fb7c902bde74dbf8586840535add000":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J-AAmPzqdHEK","executionInfo":{"status":"ok","timestamp":1650959422954,"user_tz":-540,"elapsed":5951,"user":{"displayName":"‍신지섭(학부생-빅데이터경영통계전공)","userId":"03536684182824485899"}},"outputId":"9011e8e7-9102-4f34-cc07-b6db027f9ddb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.18.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.5.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jl9x-EIRdOoz","executionInfo":{"status":"ok","timestamp":1650959431011,"user_tz":-540,"elapsed":2261,"user":{"displayName":"‍신지섭(학부생-빅데이터경영통계전공)","userId":"03536684182824485899"}},"outputId":"4e92cd14-6e6a-4aa3-f4ee-57deb7887eb5"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from tqdm.notebook import tqdm\n","import warnings\n","from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n","from torch import torch\n","from sklearn.model_selection import train_test_split\n","warnings.filterwarnings(action='ignore')\n","%matplotlib inline"],"metadata":{"id":"cDaYcrSve_wD","executionInfo":{"status":"ok","timestamp":1650959879233,"user_tz":-540,"elapsed":324,"user":{"displayName":"‍신지섭(학부생-빅데이터경영통계전공)","userId":"03536684182824485899"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["# 1. 모델을 불러옵니다."],"metadata":{"id":"nzirLQGrfeOu"}},{"cell_type":"code","source":["from transformers import AutoModel, AutoTokenizer\n","\n","model = AutoModel.from_pretrained(\"klue/roberta-base\")\n","tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")\n","\n","model.pooler.dense = nn.Linear(768, 1)\n","model.pooler.activation = nn.Sigmoid()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xv9tCtihfAZu","executionInfo":{"status":"ok","timestamp":1650960160700,"user_tz":-540,"elapsed":2914,"user":{"displayName":"‍신지섭(학부생-빅데이터경영통계전공)","userId":"03536684182824485899"}},"outputId":"6ed48e63-6f8f-4ae0-c5e4-b482e25caaf8"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["# 2. 모두에게 동일하게 적용될 테스트셋을 가져옵니다."],"metadata":{"id":"Z3TGF5-vfgbF"}},{"cell_type":"code","source":["class LoadDataset(Dataset):\n","    def __init__(self, df, tk):\n","        self.df = df\n","        self.tokenizer = tk\n","\n","    def __len__(self):\n","        return len(self.df)\n","  \n","    def __getitem__(self, idx):\n","        row = self.df.iloc[idx, :].values\n","        # target이 없는경우 (즉, 문장만 입력된 경우)\n","        if len(row) <= 1:\n","            text = row[0]\n","\n","            inputs = self.tokenizer(\n","                text, \n","                return_tensors='pt',\n","                truncation=True,\n","                max_length=50,\n","                pad_to_max_length=True,\n","                add_special_tokens=True\n","                )\n","            \n","            input_ids = inputs['input_ids'][0]\n","            attention_mask = inputs['attention_mask'][0]\n","\n","            return input_ids, attention_mask     \n","            \n","        # target이 있는 경우 (원래 코드)\n","        else:\n","            text = row[0]\n","            y = row[1]\n","\n","            inputs = self.tokenizer(\n","                text, \n","                return_tensors='pt',\n","                truncation=True,\n","                max_length=50,\n","                pad_to_max_length=True,\n","                add_special_tokens=True\n","                )\n","            \n","            input_ids = inputs['input_ids'][0]\n","            attention_mask = inputs['attention_mask'][0]\n","\n","            return input_ids, attention_mask, y"],"metadata":{"id":"_wgGsYfKfTqa","executionInfo":{"status":"ok","timestamp":1650960165861,"user_tz":-540,"elapsed":298,"user":{"displayName":"‍신지섭(학부생-빅데이터경영통계전공)","userId":"03536684182824485899"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["# 테스트용 데이터를 불러옵니다.\n","PATH = '/content/gdrive/MyDrive/A3_datasets/KoElectra/koelectra_datasets/'\n","cate = '종교'\n","df = pd.read_csv(PATH + f'{cate}.csv')\n","df = df[['문장', f'{cate}']]\n","\n","def dataSplit(dataset, y_label):\n","  X_train, X_val= train_test_split(dataset, test_size = 0.2, stratify = dataset[y_label], random_state =427)\n","  return X_train, X_val\n","\n","X_train, X_test = dataSplit(df, cate)"],"metadata":{"id":"_0aczF1zfAXY","executionInfo":{"status":"ok","timestamp":1650960169479,"user_tz":-540,"elapsed":744,"user":{"displayName":"‍신지섭(학부생-빅데이터경영통계전공)","userId":"03536684182824485899"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["test_set = LoadDataset(X_test, tokenizer) # 이 테스트셋은 unsmile.csv로만 전이학습한 모델의 평가데이터와 동일합니다."],"metadata":{"id":"1xfCePxtfAUx","executionInfo":{"status":"ok","timestamp":1650960169479,"user_tz":-540,"elapsed":6,"user":{"displayName":"‍신지섭(학부생-빅데이터경영통계전공)","userId":"03536684182824485899"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["# 3. 추가로 전이학습할 카카오 데이터 불러오기"],"metadata":{"id":"y_mvY7F4fwhl"}},{"cell_type":"code","source":["# 카카오 데이터를 불러옵니다.\n","PATH = '/content/gdrive/MyDrive/A3_datasets/sudo_labeling/kakao/'\n","kakao_df = pd.read_csv(PATH + f'{cate}.csv')\n","kakao_df = kakao_df[['text',f'{cate}']]\n","\n","X_train, X_valid = dataSplit(kakao_df, cate)\n","\n","train_set = LoadDataset(X_train, tokenizer)\n","val_set = LoadDataset(X_valid, tokenizer)"],"metadata":{"id":"WdwKXCGSfASV","executionInfo":{"status":"ok","timestamp":1650960169479,"user_tz":-540,"elapsed":6,"user":{"displayName":"‍신지섭(학부생-빅데이터경영통계전공)","userId":"03536684182824485899"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\")\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YrH9tJaTfAP6","executionInfo":{"status":"ok","timestamp":1650960170059,"user_tz":-540,"elapsed":586,"user":{"displayName":"‍신지섭(학부생-빅데이터경영통계전공)","userId":"03536684182824485899"}},"outputId":"c81fe1b4-12a9-47a7-925d-7437ad741b25"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RobertaModel(\n","  (embeddings): RobertaEmbeddings(\n","    (word_embeddings): Embedding(32000, 768, padding_idx=1)\n","    (position_embeddings): Embedding(514, 768, padding_idx=1)\n","    (token_type_embeddings): Embedding(1, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): RobertaEncoder(\n","    (layer): ModuleList(\n","      (0): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): RobertaPooler(\n","    (dense): Linear(in_features=768, out_features=1, bias=True)\n","    (activation): Sigmoid()\n","  )\n",")"]},"metadata":{},"execution_count":56}]},{"cell_type":"markdown","source":["# 4. 카카오 데이터로 추가학습 train"],"metadata":{"id":"QKf6miILhhyE"}},{"cell_type":"code","source":["model.load_state_dict(torch.load(f'/content/gdrive/MyDrive/A3_datasets/roberta/checkpoint_{cate}.pt'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3U7CmYkGjXxL","executionInfo":{"status":"ok","timestamp":1650960176921,"user_tz":-540,"elapsed":6865,"user":{"displayName":"‍신지섭(학부생-빅데이터경영통계전공)","userId":"03536684182824485899"}},"outputId":"233c20fa-527d-4610-e2a8-d2e58aa2a05d"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["from transformers import get_cosine_schedule_with_warmup, AdamW\n","\n","epochs = 300 # epochs 증가\n","batch_size = 64 # batch size 감소\n","warmup_ratio=0.1\n","t_total = len(train_set) * epochs\n","optimizer = AdamW(model.parameters(), lr=1e-5, eps = 1e-8) # lr 1/10으로 변경\n","train_loader = DataLoader(train_set, batch_size=batch_size)\n","val_loader = DataLoader(val_set, batch_size=batch_size) # val loader 추가\n","test_loader = DataLoader(test_set, batch_size = batch_size) # test loader 추가\n","loss_f = nn.BCEWithLogitsLoss() # loss f 변경\n","\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=1, num_training_steps=t_total)"],"metadata":{"id":"HaZVQXs6fANK","executionInfo":{"status":"ok","timestamp":1650960176921,"user_tz":-540,"elapsed":5,"user":{"displayName":"‍신지섭(학부생-빅데이터경영통계전공)","userId":"03536684182824485899"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["# https://github.com/Bjarten/early-stopping-pytorch\n","class EarlyStopping:\n","    def __init__(self, patience=7, verbose=False, delta=0, path=f'/content/gdrive/MyDrive/A3_datasets/roberta/checkpoint_{cate}_kakao.pt'):\n","        \"\"\"\n","        Args:\n","            patience (int): validation loss가 개선된 후 기다리는 기간\n","                            Default: 7\n","            verbose (bool): True일 경우 각 validation loss의 개선 사항 메세지 출력\n","                            Default: False\n","            delta (float): 개선되었다고 인정되는 monitered quantity의 최소 변화\n","                            Default: 0\n","            path (str): checkpoint저장 경로\n","                            Default: 'checkpoint.pt'\n","        \"\"\"\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        if self.verbose:\n","            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss"],"metadata":{"id":"VvHIrG2QfALD","executionInfo":{"status":"ok","timestamp":1650960176922,"user_tz":-540,"elapsed":6,"user":{"displayName":"‍신지섭(학부생-빅데이터경영통계전공)","userId":"03536684182824485899"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["early_stopping = EarlyStopping(patience = 7, verbose = True)"],"metadata":{"id":"ZDT1gcc7fAH6","executionInfo":{"status":"ok","timestamp":1650960176922,"user_tz":-540,"elapsed":5,"user":{"displayName":"‍신지섭(학부생-빅데이터경영통계전공)","userId":"03536684182824485899"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\n","\n","for i in tqdm(range(epochs)):\n","    train_loss_list = [] # 변수 변경\n","    val_loss_list = []\n","    val_score_list = []\n","\n","    epoch_train_loss = []\n","    epoch_val_loss = []\n","    epoch_val_score = []\n","    # train\n","    model.train()\n","    for input_ids_batch, attention_masks_batch, y_batch in train_loader:\n","        input_ids_batch = input_ids_batch.to(device)\n","        attention_masks_batch = attention_masks_batch.to(device)\n","        y_batch = y_batch.to(device)\n","        optimizer.zero_grad()\n","        y_pred = model(input_ids_batch, attention_mask=attention_masks_batch).pooler_output.reshape(-1)\n","#        print(y_pred)\n","        loss = loss_f(y_pred.type(torch.FloatTensor), y_batch.type(torch.FloatTensor))\n","#        print(loss)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        train_loss_list.append(loss.item())\n","\n","    # validation loss\n","    model.eval()\n","    for input_ids_batch_val, attention_masks_batch_val, y_batch_val in val_loader:\n","        input_ids_batch_val = input_ids_batch_val.to(device)\n","        attention_masks_batch_val = attention_masks_batch_val.to(device)\n","        y_batch_val = y_batch_val.to(device)\n","        y_pred_val = model(input_ids_batch_val, attention_mask = attention_masks_batch_val).pooler_output.reshape(-1)\n","        loss = loss_f(y_pred_val.type(torch.FloatTensor), y_batch_val.type(torch.FloatTensor))\n","        val_score = roc_auc_score(y_batch_val.tolist(), y_pred_val.tolist())\n","        val_loss_list.append(loss.item())\n","        val_score_list.append(val_score)\n","\n","    # epoch당 loss 계산 (for early stopping)\n","    train_loss = np.average(train_loss_list)\n","    val_loss = np.average(val_loss_list)\n","    val_score = np.average(val_score_list)\n","\n","    epoch_train_loss.append(train_loss)\n","    epoch_val_loss.append(val_loss)\n","    epoch_val_score.append(val_score)\n","    epoch_len = len(str(epochs))\n","\n","    print_msg = (f'[{i:>{epoch_len}}/{epochs:>{epoch_len}}] ' +\n","                 f'train_loss: {train_loss:.5f} ' +\n","                 f'valid_loss: {val_loss:.5f} ' +\n","                 f'valid_score: {val_score:.5f}')\n","\n","    print(print_msg)\n","    \n","    # clear lists to track next epoch\n","    train_loss_list = []\n","    val_loss_list = []\n","    val_score_list = []\n","    early_stopping(val_loss, model)\n","    if early_stopping.early_stop:\n","        print('early stopping')\n","        break\n","    \n","model.load_state_dict(torch.load(f'/content/gdrive/MyDrive/A3_datasets/roberta/checkpoint_{cate}_kakao.pt'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["959d71a293b54714b33a12c0275c5a06","ef800a642e9448c1b75c2bbed640a4bb","992736e6a6a846e484d0dfca5a0e517c","931ce25b47b84659abff4554c90b02e0","5c0cc41806de4341822c60ca72af02f2","35446649e9af4501b39a25a4fb4db86c","67ac93e37d42457faf3c28979120da25","789e8061d1684b298c221eeb0ea6322d","ae2de34d8f634bffafad6c89eaa61389","40aa9c3042654b4c8058278c6707ff79","6fb7c902bde74dbf8586840535add000"]},"id":"vkt2muGyhukJ","executionInfo":{"status":"error","timestamp":1650960810320,"user_tz":-540,"elapsed":633403,"user":{"displayName":"‍신지섭(학부생-빅데이터경영통계전공)","userId":"03536684182824485899"}},"outputId":"4f30768e-7575-49b4-9390-8968da912101"},"execution_count":61,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/300 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"959d71a293b54714b33a12c0275c5a06"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[  0/300] train_loss: 0.50660 valid_loss: 0.51094 valid_score: 1.00000\n","Validation loss decreased (inf --> 0.510943).  Saving model ...\n","[  1/300] train_loss: 0.50555 valid_loss: 0.50730 valid_score: 1.00000\n","Validation loss decreased (0.510943 --> 0.507302).  Saving model ...\n","[  2/300] train_loss: 0.51597 valid_loss: 0.50614 valid_score: 1.00000\n","Validation loss decreased (0.507302 --> 0.506136).  Saving model ...\n","[  3/300] train_loss: 0.50564 valid_loss: 0.50554 valid_score: 1.00000\n","Validation loss decreased (0.506136 --> 0.505537).  Saving model ...\n","[  4/300] train_loss: 0.50472 valid_loss: 0.50518 valid_score: 1.00000\n","Validation loss decreased (0.505537 --> 0.505185).  Saving model ...\n","[  5/300] train_loss: 0.50472 valid_loss: 0.50495 valid_score: 1.00000\n","Validation loss decreased (0.505185 --> 0.504952).  Saving model ...\n","[  6/300] train_loss: 0.50462 valid_loss: 0.50479 valid_score: 1.00000\n","Validation loss decreased (0.504952 --> 0.504787).  Saving model ...\n","[  7/300] train_loss: 0.50462 valid_loss: 0.50466 valid_score: 1.00000\n","Validation loss decreased (0.504787 --> 0.504662).  Saving model ...\n","[  8/300] train_loss: 0.50451 valid_loss: 0.50456 valid_score: 1.00000\n","Validation loss decreased (0.504662 --> 0.504562).  Saving model ...\n","[  9/300] train_loss: 0.50436 valid_loss: 0.50448 valid_score: 1.00000\n","Validation loss decreased (0.504562 --> 0.504481).  Saving model ...\n","[ 10/300] train_loss: 0.50438 valid_loss: 0.50441 valid_score: 1.00000\n","Validation loss decreased (0.504481 --> 0.504412).  Saving model ...\n","[ 11/300] train_loss: 0.50429 valid_loss: 0.50435 valid_score: 1.00000\n","Validation loss decreased (0.504412 --> 0.504352).  Saving model ...\n","[ 12/300] train_loss: 0.50463 valid_loss: 0.50430 valid_score: 1.00000\n","Validation loss decreased (0.504352 --> 0.504299).  Saving model ...\n","[ 13/300] train_loss: 0.50432 valid_loss: 0.50425 valid_score: 1.00000\n","Validation loss decreased (0.504299 --> 0.504251).  Saving model ...\n","[ 14/300] train_loss: 0.50447 valid_loss: 0.50421 valid_score: 1.00000\n","Validation loss decreased (0.504251 --> 0.504207).  Saving model ...\n","[ 15/300] train_loss: 0.50432 valid_loss: 0.50417 valid_score: 1.00000\n","Validation loss decreased (0.504207 --> 0.504167).  Saving model ...\n","[ 16/300] train_loss: 0.50428 valid_loss: 0.50413 valid_score: 1.00000\n","Validation loss decreased (0.504167 --> 0.504130).  Saving model ...\n","[ 17/300] train_loss: 0.50418 valid_loss: 0.50410 valid_score: 1.00000\n","Validation loss decreased (0.504130 --> 0.504096).  Saving model ...\n","[ 18/300] train_loss: 0.50416 valid_loss: 0.50406 valid_score: 1.00000\n","Validation loss decreased (0.504096 --> 0.504065).  Saving model ...\n","[ 19/300] train_loss: 0.50408 valid_loss: 0.50404 valid_score: 1.00000\n","Validation loss decreased (0.504065 --> 0.504036).  Saving model ...\n","[ 20/300] train_loss: 0.50408 valid_loss: 0.50401 valid_score: 1.00000\n","Validation loss decreased (0.504036 --> 0.504009).  Saving model ...\n","[ 21/300] train_loss: 0.50405 valid_loss: 0.50398 valid_score: 1.00000\n","Validation loss decreased (0.504009 --> 0.503984).  Saving model ...\n","[ 22/300] train_loss: 0.50416 valid_loss: 0.50396 valid_score: 1.00000\n","Validation loss decreased (0.503984 --> 0.503960).  Saving model ...\n","[ 23/300] train_loss: 0.50399 valid_loss: 0.50394 valid_score: 1.00000\n","Validation loss decreased (0.503960 --> 0.503937).  Saving model ...\n","[ 24/300] train_loss: 0.50405 valid_loss: 0.50392 valid_score: 1.00000\n","Validation loss decreased (0.503937 --> 0.503916).  Saving model ...\n","[ 25/300] train_loss: 0.50433 valid_loss: 0.50390 valid_score: 1.00000\n","Validation loss decreased (0.503916 --> 0.503896).  Saving model ...\n","[ 26/300] train_loss: 0.50398 valid_loss: 0.50388 valid_score: 1.00000\n","Validation loss decreased (0.503896 --> 0.503877).  Saving model ...\n","[ 27/300] train_loss: 0.50394 valid_loss: 0.50386 valid_score: 1.00000\n","Validation loss decreased (0.503877 --> 0.503859).  Saving model ...\n","[ 28/300] train_loss: 0.50391 valid_loss: 0.50384 valid_score: 1.00000\n","Validation loss decreased (0.503859 --> 0.503843).  Saving model ...\n","[ 29/300] train_loss: 0.50391 valid_loss: 0.50383 valid_score: 1.00000\n","Validation loss decreased (0.503843 --> 0.503827).  Saving model ...\n","[ 30/300] train_loss: 0.50399 valid_loss: 0.50381 valid_score: 1.00000\n","Validation loss decreased (0.503827 --> 0.503812).  Saving model ...\n","[ 31/300] train_loss: 0.50385 valid_loss: 0.50380 valid_score: 1.00000\n","Validation loss decreased (0.503812 --> 0.503798).  Saving model ...\n","[ 32/300] train_loss: 0.50388 valid_loss: 0.50379 valid_score: 1.00000\n","Validation loss decreased (0.503798 --> 0.503785).  Saving model ...\n","[ 33/300] train_loss: 0.50384 valid_loss: 0.50377 valid_score: 1.00000\n","Validation loss decreased (0.503785 --> 0.503773).  Saving model ...\n","[ 34/300] train_loss: 0.50384 valid_loss: 0.50376 valid_score: 1.00000\n","Validation loss decreased (0.503773 --> 0.503761).  Saving model ...\n","[ 35/300] train_loss: 0.50380 valid_loss: 0.50375 valid_score: 1.00000\n","Validation loss decreased (0.503761 --> 0.503750).  Saving model ...\n","[ 36/300] train_loss: 0.50390 valid_loss: 0.50374 valid_score: 1.00000\n","Validation loss decreased (0.503750 --> 0.503739).  Saving model ...\n","[ 37/300] train_loss: 0.50392 valid_loss: 0.50373 valid_score: 1.00000\n","Validation loss decreased (0.503739 --> 0.503729).  Saving model ...\n","[ 38/300] train_loss: 0.50378 valid_loss: 0.50372 valid_score: 1.00000\n","Validation loss decreased (0.503729 --> 0.503719).  Saving model ...\n","[ 39/300] train_loss: 0.50380 valid_loss: 0.50371 valid_score: 1.00000\n","Validation loss decreased (0.503719 --> 0.503710).  Saving model ...\n","[ 40/300] train_loss: 0.50381 valid_loss: 0.50370 valid_score: 1.00000\n","Validation loss decreased (0.503710 --> 0.503701).  Saving model ...\n","[ 41/300] train_loss: 0.50405 valid_loss: 0.50369 valid_score: 1.00000\n","Validation loss decreased (0.503701 --> 0.503692).  Saving model ...\n","[ 42/300] train_loss: 0.50386 valid_loss: 0.50368 valid_score: 1.00000\n","Validation loss decreased (0.503692 --> 0.503684).  Saving model ...\n","[ 43/300] train_loss: 0.50377 valid_loss: 0.50368 valid_score: 1.00000\n","Validation loss decreased (0.503684 --> 0.503676).  Saving model ...\n","[ 44/300] train_loss: 0.50379 valid_loss: 0.50367 valid_score: 1.00000\n","Validation loss decreased (0.503676 --> 0.503668).  Saving model ...\n","[ 45/300] train_loss: 0.50374 valid_loss: 0.50366 valid_score: 1.00000\n","Validation loss decreased (0.503668 --> 0.503661).  Saving model ...\n","[ 46/300] train_loss: 0.50372 valid_loss: 0.50365 valid_score: 1.00000\n","Validation loss decreased (0.503661 --> 0.503654).  Saving model ...\n","[ 47/300] train_loss: 0.50369 valid_loss: 0.50365 valid_score: 1.00000\n","Validation loss decreased (0.503654 --> 0.503648).  Saving model ...\n","[ 48/300] train_loss: 0.50373 valid_loss: 0.50364 valid_score: 1.00000\n","Validation loss decreased (0.503648 --> 0.503642).  Saving model ...\n","[ 49/300] train_loss: 0.50373 valid_loss: 0.50364 valid_score: 1.00000\n","Validation loss decreased (0.503642 --> 0.503636).  Saving model ...\n","[ 50/300] train_loss: 0.50372 valid_loss: 0.50363 valid_score: 1.00000\n","Validation loss decreased (0.503636 --> 0.503630).  Saving model ...\n","[ 51/300] train_loss: 0.50367 valid_loss: 0.50362 valid_score: 1.00000\n","Validation loss decreased (0.503630 --> 0.503624).  Saving model ...\n","[ 52/300] train_loss: 0.50370 valid_loss: 0.50362 valid_score: 1.00000\n","Validation loss decreased (0.503624 --> 0.503619).  Saving model ...\n","[ 53/300] train_loss: 0.50368 valid_loss: 0.50361 valid_score: 1.00000\n","Validation loss decreased (0.503619 --> 0.503614).  Saving model ...\n","[ 54/300] train_loss: 0.50368 valid_loss: 0.50361 valid_score: 1.00000\n","Validation loss decreased (0.503614 --> 0.503609).  Saving model ...\n","[ 55/300] train_loss: 0.50369 valid_loss: 0.50360 valid_score: 1.00000\n","Validation loss decreased (0.503609 --> 0.503604).  Saving model ...\n","[ 56/300] train_loss: 0.50375 valid_loss: 0.50360 valid_score: 1.00000\n","Validation loss decreased (0.503604 --> 0.503599).  Saving model ...\n","[ 57/300] train_loss: 0.50385 valid_loss: 0.50359 valid_score: 1.00000\n","Validation loss decreased (0.503599 --> 0.503595).  Saving model ...\n","[ 58/300] train_loss: 0.50365 valid_loss: 0.50359 valid_score: 1.00000\n","Validation loss decreased (0.503595 --> 0.503590).  Saving model ...\n","[ 59/300] train_loss: 0.50388 valid_loss: 0.50359 valid_score: 1.00000\n","Validation loss decreased (0.503590 --> 0.503586).  Saving model ...\n","[ 60/300] train_loss: 0.50377 valid_loss: 0.50358 valid_score: 1.00000\n","Validation loss decreased (0.503586 --> 0.503582).  Saving model ...\n","[ 61/300] train_loss: 0.50389 valid_loss: 0.50358 valid_score: 1.00000\n","Validation loss decreased (0.503582 --> 0.503577).  Saving model ...\n","[ 62/300] train_loss: 0.50361 valid_loss: 0.50357 valid_score: 1.00000\n","Validation loss decreased (0.503577 --> 0.503573).  Saving model ...\n","[ 63/300] train_loss: 0.50364 valid_loss: 0.50357 valid_score: 1.00000\n","Validation loss decreased (0.503573 --> 0.503569).  Saving model ...\n","[ 64/300] train_loss: 0.50366 valid_loss: 0.50357 valid_score: 1.00000\n","Validation loss decreased (0.503569 --> 0.503566).  Saving model ...\n","[ 65/300] train_loss: 0.50365 valid_loss: 0.50356 valid_score: 1.00000\n","Validation loss decreased (0.503566 --> 0.503562).  Saving model ...\n","[ 66/300] train_loss: 0.50364 valid_loss: 0.50356 valid_score: 1.00000\n","Validation loss decreased (0.503562 --> 0.503558).  Saving model ...\n","[ 67/300] train_loss: 0.50361 valid_loss: 0.50355 valid_score: 1.00000\n","Validation loss decreased (0.503558 --> 0.503555).  Saving model ...\n","[ 68/300] train_loss: 0.50363 valid_loss: 0.50355 valid_score: 1.00000\n","Validation loss decreased (0.503555 --> 0.503552).  Saving model ...\n","[ 69/300] train_loss: 0.50362 valid_loss: 0.50355 valid_score: 1.00000\n","Validation loss decreased (0.503552 --> 0.503549).  Saving model ...\n","[ 70/300] train_loss: 0.50367 valid_loss: 0.50355 valid_score: 1.00000\n","Validation loss decreased (0.503549 --> 0.503545).  Saving model ...\n","[ 71/300] train_loss: 0.50358 valid_loss: 0.50354 valid_score: 1.00000\n","Validation loss decreased (0.503545 --> 0.503542).  Saving model ...\n","[ 72/300] train_loss: 0.50361 valid_loss: 0.50354 valid_score: 1.00000\n","Validation loss decreased (0.503542 --> 0.503540).  Saving model ...\n","[ 73/300] train_loss: 0.50367 valid_loss: 0.50354 valid_score: 1.00000\n","Validation loss decreased (0.503540 --> 0.503537).  Saving model ...\n","[ 74/300] train_loss: 0.50369 valid_loss: 0.50353 valid_score: 1.00000\n","Validation loss decreased (0.503537 --> 0.503534).  Saving model ...\n","[ 75/300] train_loss: 0.50359 valid_loss: 0.50353 valid_score: 1.00000\n","Validation loss decreased (0.503534 --> 0.503531).  Saving model ...\n","[ 76/300] train_loss: 0.50360 valid_loss: 0.50353 valid_score: 1.00000\n","Validation loss decreased (0.503531 --> 0.503529).  Saving model ...\n","[ 77/300] train_loss: 0.50357 valid_loss: 0.50353 valid_score: 1.00000\n","Validation loss decreased (0.503529 --> 0.503526).  Saving model ...\n","[ 78/300] train_loss: 0.50357 valid_loss: 0.50352 valid_score: 1.00000\n","Validation loss decreased (0.503526 --> 0.503524).  Saving model ...\n","[ 79/300] train_loss: 0.50359 valid_loss: 0.50352 valid_score: 1.00000\n","Validation loss decreased (0.503524 --> 0.503521).  Saving model ...\n","[ 80/300] train_loss: 0.50360 valid_loss: 0.50352 valid_score: 1.00000\n","Validation loss decreased (0.503521 --> 0.503519).  Saving model ...\n","[ 81/300] train_loss: 0.50357 valid_loss: 0.50352 valid_score: 1.00000\n","Validation loss decreased (0.503519 --> 0.503517).  Saving model ...\n","[ 82/300] train_loss: 0.50360 valid_loss: 0.50351 valid_score: 1.00000\n","Validation loss decreased (0.503517 --> 0.503515).  Saving model ...\n","[ 83/300] train_loss: 0.50359 valid_loss: 0.50351 valid_score: 1.00000\n","Validation loss decreased (0.503515 --> 0.503512).  Saving model ...\n","[ 84/300] train_loss: 0.50356 valid_loss: 0.50351 valid_score: 1.00000\n","Validation loss decreased (0.503512 --> 0.503510).  Saving model ...\n","[ 85/300] train_loss: 0.50357 valid_loss: 0.50351 valid_score: 1.00000\n","Validation loss decreased (0.503510 --> 0.503508).  Saving model ...\n","[ 86/300] train_loss: 0.50354 valid_loss: 0.50351 valid_score: 1.00000\n","Validation loss decreased (0.503508 --> 0.503506).  Saving model ...\n","[ 87/300] train_loss: 0.50356 valid_loss: 0.50350 valid_score: 1.00000\n","Validation loss decreased (0.503506 --> 0.503504).  Saving model ...\n","[ 88/300] train_loss: 0.50354 valid_loss: 0.50350 valid_score: 1.00000\n","Validation loss decreased (0.503504 --> 0.503502).  Saving model ...\n","[ 89/300] train_loss: 0.50360 valid_loss: 0.50350 valid_score: 1.00000\n","Validation loss decreased (0.503502 --> 0.503500).  Saving model ...\n","[ 90/300] train_loss: 0.50357 valid_loss: 0.50350 valid_score: 1.00000\n","Validation loss decreased (0.503500 --> 0.503498).  Saving model ...\n","[ 91/300] train_loss: 0.50359 valid_loss: 0.50350 valid_score: 1.00000\n","Validation loss decreased (0.503498 --> 0.503496).  Saving model ...\n","[ 92/300] train_loss: 0.50353 valid_loss: 0.50349 valid_score: 1.00000\n","Validation loss decreased (0.503496 --> 0.503494).  Saving model ...\n","[ 93/300] train_loss: 0.50353 valid_loss: 0.50349 valid_score: 1.00000\n","Validation loss decreased (0.503494 --> 0.503493).  Saving model ...\n","[ 94/300] train_loss: 0.50360 valid_loss: 0.50349 valid_score: 1.00000\n","Validation loss decreased (0.503493 --> 0.503491).  Saving model ...\n","[ 95/300] train_loss: 0.50354 valid_loss: 0.50349 valid_score: 1.00000\n","Validation loss decreased (0.503491 --> 0.503489).  Saving model ...\n","[ 96/300] train_loss: 0.50354 valid_loss: 0.50349 valid_score: 1.00000\n","Validation loss decreased (0.503489 --> 0.503487).  Saving model ...\n","[ 97/300] train_loss: 0.50355 valid_loss: 0.50349 valid_score: 1.00000\n","Validation loss decreased (0.503487 --> 0.503486).  Saving model ...\n","[ 98/300] train_loss: 0.50354 valid_loss: 0.50348 valid_score: 1.00000\n","Validation loss decreased (0.503486 --> 0.503484).  Saving model ...\n","[ 99/300] train_loss: 0.50358 valid_loss: 0.50348 valid_score: 1.00000\n","Validation loss decreased (0.503484 --> 0.503482).  Saving model ...\n","[100/300] train_loss: 0.50363 valid_loss: 0.50348 valid_score: 1.00000\n","Validation loss decreased (0.503482 --> 0.503481).  Saving model ...\n","[101/300] train_loss: 0.50351 valid_loss: 0.50348 valid_score: 1.00000\n","Validation loss decreased (0.503481 --> 0.503479).  Saving model ...\n","[102/300] train_loss: 0.50353 valid_loss: 0.50348 valid_score: 1.00000\n","Validation loss decreased (0.503479 --> 0.503478).  Saving model ...\n","[103/300] train_loss: 0.50353 valid_loss: 0.50348 valid_score: 1.00000\n","Validation loss decreased (0.503478 --> 0.503476).  Saving model ...\n","[104/300] train_loss: 0.50353 valid_loss: 0.50347 valid_score: 1.00000\n","Validation loss decreased (0.503476 --> 0.503475).  Saving model ...\n","[105/300] train_loss: 0.50354 valid_loss: 0.50347 valid_score: 1.00000\n","Validation loss decreased (0.503475 --> 0.503473).  Saving model ...\n","[106/300] train_loss: 0.50356 valid_loss: 0.50347 valid_score: 1.00000\n","Validation loss decreased (0.503473 --> 0.503472).  Saving model ...\n","[107/300] train_loss: 0.50352 valid_loss: 0.50347 valid_score: 1.00000\n","Validation loss decreased (0.503472 --> 0.503470).  Saving model ...\n","[108/300] train_loss: 0.50353 valid_loss: 0.50347 valid_score: 1.00000\n","Validation loss decreased (0.503470 --> 0.503469).  Saving model ...\n","[109/300] train_loss: 0.50352 valid_loss: 0.50347 valid_score: 1.00000\n","Validation loss decreased (0.503469 --> 0.503468).  Saving model ...\n","[110/300] train_loss: 0.50353 valid_loss: 0.50347 valid_score: 1.00000\n","Validation loss decreased (0.503468 --> 0.503466).  Saving model ...\n","[111/300] train_loss: 0.50352 valid_loss: 0.50347 valid_score: 1.00000\n","Validation loss decreased (0.503466 --> 0.503465).  Saving model ...\n","[112/300] train_loss: 0.50351 valid_loss: 0.50346 valid_score: 1.00000\n","Validation loss decreased (0.503465 --> 0.503464).  Saving model ...\n","[113/300] train_loss: 0.50353 valid_loss: 0.50346 valid_score: 1.00000\n","Validation loss decreased (0.503464 --> 0.503463).  Saving model ...\n","[114/300] train_loss: 0.50350 valid_loss: 0.50346 valid_score: 1.00000\n","Validation loss decreased (0.503463 --> 0.503461).  Saving model ...\n","[115/300] train_loss: 0.50351 valid_loss: 0.50346 valid_score: 1.00000\n","Validation loss decreased (0.503461 --> 0.503460).  Saving model ...\n","[116/300] train_loss: 0.50354 valid_loss: 0.50346 valid_score: 1.00000\n","Validation loss decreased (0.503460 --> 0.503459).  Saving model ...\n","[117/300] train_loss: 0.50355 valid_loss: 0.50346 valid_score: 1.00000\n","Validation loss decreased (0.503459 --> 0.503458).  Saving model ...\n","[118/300] train_loss: 0.50356 valid_loss: 0.50346 valid_score: 1.00000\n","Validation loss decreased (0.503458 --> 0.503457).  Saving model ...\n","[119/300] train_loss: 0.50350 valid_loss: 0.50346 valid_score: 1.00000\n","Validation loss decreased (0.503457 --> 0.503455).  Saving model ...\n","[120/300] train_loss: 0.50351 valid_loss: 0.50345 valid_score: 1.00000\n","Validation loss decreased (0.503455 --> 0.503454).  Saving model ...\n","[121/300] train_loss: 0.50353 valid_loss: 0.50345 valid_score: 1.00000\n","Validation loss decreased (0.503454 --> 0.503453).  Saving model ...\n","[122/300] train_loss: 0.50352 valid_loss: 0.50345 valid_score: 1.00000\n","Validation loss decreased (0.503453 --> 0.503452).  Saving model ...\n","[123/300] train_loss: 0.50353 valid_loss: 0.50345 valid_score: 1.00000\n","Validation loss decreased (0.503452 --> 0.503451).  Saving model ...\n","[124/300] train_loss: 0.50350 valid_loss: 0.50345 valid_score: 1.00000\n","Validation loss decreased (0.503451 --> 0.503450).  Saving model ...\n","[125/300] train_loss: 0.50349 valid_loss: 0.50345 valid_score: 1.00000\n","Validation loss decreased (0.503450 --> 0.503449).  Saving model ...\n","[126/300] train_loss: 0.50355 valid_loss: 0.50345 valid_score: 1.00000\n","Validation loss decreased (0.503449 --> 0.503448).  Saving model ...\n","[127/300] train_loss: 0.50352 valid_loss: 0.50345 valid_score: 1.00000\n","Validation loss decreased (0.503448 --> 0.503447).  Saving model ...\n","[128/300] train_loss: 0.50352 valid_loss: 0.50345 valid_score: 1.00000\n","Validation loss decreased (0.503447 --> 0.503445).  Saving model ...\n","[129/300] train_loss: 0.50360 valid_loss: 0.50344 valid_score: 1.00000\n","Validation loss decreased (0.503445 --> 0.503444).  Saving model ...\n","[130/300] train_loss: 0.50351 valid_loss: 0.50344 valid_score: 1.00000\n","Validation loss decreased (0.503444 --> 0.503443).  Saving model ...\n","[131/300] train_loss: 0.50349 valid_loss: 0.50344 valid_score: 1.00000\n","Validation loss decreased (0.503443 --> 0.503442).  Saving model ...\n","[132/300] train_loss: 0.50349 valid_loss: 0.50344 valid_score: 1.00000\n","Validation loss decreased (0.503442 --> 0.503441).  Saving model ...\n","[133/300] train_loss: 0.50352 valid_loss: 0.50344 valid_score: 1.00000\n","Validation loss decreased (0.503441 --> 0.503440).  Saving model ...\n","[134/300] train_loss: 0.50351 valid_loss: 0.50344 valid_score: 1.00000\n","Validation loss decreased (0.503440 --> 0.503439).  Saving model ...\n","[135/300] train_loss: 0.50349 valid_loss: 0.50344 valid_score: 1.00000\n","Validation loss decreased (0.503439 --> 0.503438).  Saving model ...\n","[136/300] train_loss: 0.50350 valid_loss: 0.50344 valid_score: 1.00000\n","Validation loss decreased (0.503438 --> 0.503437).  Saving model ...\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-61-047dca0af1d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mval_loss_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mval_score_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mearly_stopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'early stopping'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-59-7d136b5f5e96>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, val_loss, model)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-59-7d136b5f5e96>\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, val_loss, model)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loss_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:300] . unexpected pos 411832704 vs 411832592"]}]},{"cell_type":"markdown","source":["# 5. 스코어 확인하기"],"metadata":{"id":"YMHcOxj9iaJz"}},{"cell_type":"code","source":["model.eval()\n","score_list = []\n","for input_ids_batch, attention_masks_batch, y_batch in tqdm(test_loader):\n","    input_ids_batch = input_ids_batch.to(device)\n","    attention_masks_batch = attention_masks_batch.to(device)\n","    y_batch = y_batch.to(device)\n","    y_pred = model(input_ids_batch, attention_mask=attention_masks_batch).pooler_output.reshape(-1)\n","#    print(y_pred)\n","    try:\n","        score = roc_auc_score(y_batch.tolist(), y_pred.tolist())\n","        score_list.append(score)\n","    except: pass\n","print('cate : ', cate)\n","print(\"epoch roc_auc:\", np.mean(score_list))"],"metadata":{"id":"5CIaKKrbiVsZ","executionInfo":{"status":"aborted","timestamp":1650960810320,"user_tz":-540,"elapsed":5,"user":{"displayName":"‍신지섭(학부생-빅데이터경영통계전공)","userId":"03536684182824485899"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"P_hNx55IhuYP"}}]}